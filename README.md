# Reconhecimento do Alfabeto de LIBRAS com MediaPipe e LSTM

![LicenÃ§a](https://img.shields.io/badge/licenÃ§a-MIT-blue.svg)
![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange?logo=tensorflow&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green?logo=opencv&logoColor=white)

> Este projeto Ã© referente ao meu Trabalho de ConclusÃ£o de Curso para CiÃªncia da ComputaÃ§Ã£o na Universidade Estadual de Santa Cruz (UESC). Ele implementa um sistema em tempo real para o reconhecimento de letras do alfabeto da LÃ­ngua Brasileira de Sinais (LIBRAS). A soluÃ§Ã£o utiliza a biblioteca **MediaPipe** para a extraÃ§Ã£o de pontos-chave (keypoints) das mÃ£os e uma **Rede Neural Recorrente (LSTM)** para classificar as sequÃªncias de sinais capturadas por uma webcam.

---

## (DemonstraÃ§Ã£o)

![DemonstraÃ§Ã£o do Projeto](video_demonstracao.gif)

---

## ğŸ“– Ãndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [ComeÃ§ando](#-comeÃ§ando)
  - [PrÃ©-requisitos](#prÃ©-requisitos)
  - [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso](#-uso)
  - [1. Coleta de Dados](#1-coleta-de-dados)
  - [2. Treinamento do Modelo](#2-treinamento-do-modelo)
  - [3. InferÃªncia em Tempo Real](#3-inferÃªncia-em-tempo-real)
- [LicenÃ§a](#-licenÃ§a)

---

## ğŸ’» Sobre o Projeto

O objetivo principal Ã© criar uma ferramenta acessÃ­vel que possa traduzir os sinais estÃ¡ticos e dinÃ¢micos do alfabeto de LIBRAS em texto. O fluxo de trabalho do projeto Ã© o seguinte:

1.  **Captura de VÃ­deo:** Uma webcam captura o vÃ­deo do usuÃ¡rio fazendo os sinais.
2.  **ExtraÃ§Ã£o de Pontos-Chave:** O MediaPipe Holistic processa cada frame para detectar e extrair as coordenadas dos pontos das mÃ£os.
3.  **Processamento de SequÃªncias:** Os pontos-chave de 30 frames consecutivos sÃ£o agrupados para formar uma sequÃªncia, que representa um sinal completo.
4.  **Treinamento do Modelo:** Um modelo LSTM Ã© treinado para aprender a classificar cada letra do alfabeto.
5.  **Reconhecimento em Tempo Real:** O script de inferÃªncia utiliza o modelo treinado para prever a letra que estÃ¡ sendo sinalizada em tempo real.

---

## âœ¨ Tecnologias Utilizadas

As seguintes ferramentas e bibliotecas foram essenciais para a construÃ§Ã£o deste projeto:

- **[Python 3.9+](https://www.python.org/)**: Linguagem de programaÃ§Ã£o principal.
- **[TensorFlow](https://www.tensorflow.org/)**: Framework para criaÃ§Ã£o e treinamento do modelo de Deep Learning.
- **[OpenCV](https://opencv.org/)**: Para captura e processamento de imagem da webcam.
- **[MediaPipe](https://mediapipe.dev/)**: Para detecÃ§Ã£o e rastreamento de pontos-chave das mÃ£os em tempo real.
- **[Scikit-learn](https://scikit-learn.org/)**: Para avaliaÃ§Ã£o de mÃ©tricas do modelo, como a matriz de confusÃ£o.
- **[NumPy](https://numpy.org/)**: Para manipulaÃ§Ã£o eficiente de arrays e dados numÃ©ricos.
- **[Jupyter Notebook](https://jupyter.org/)**: Para o ambiente de treinamento do modelo.

---

## ğŸ“‚ Estrutura do Projeto

```
libras_CNN-RNN/
â”œâ”€â”€ main/
â”‚   â”œâ”€â”€ lstm_inference.py     # Script para inferÃªncia em tempo real com a webcam
â”‚   â””â”€â”€ train_lstm.ipynb      # Notebook para treinamento do modelo LSTM
â”œâ”€â”€ hand_capture/
â”‚   â”œâ”€â”€ utils.py              # FunÃ§Ãµes utilitÃ¡rias (extraÃ§Ã£o de keypoints, etc.)
â”‚   â””â”€â”€ video_collector.py    # Script para coletar os dados de vÃ­deo para treinamento
â”œâ”€â”€ models/                   # DiretÃ³rio para salvar os modelos treinados (.h5)
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ lstm/
â”‚       â”œâ”€â”€ train/            # Dados de treino (gerados pelo video_collector.py)
â”‚       â””â”€â”€ test/             # Dados de teste (gerados pelo video_collector.py)
â””â”€â”€ README.md                 # Este arquivo
```

---

## ğŸš€ ComeÃ§ando

Siga os passos abaixo para ter uma cÃ³pia local do projeto funcionando.

### PrÃ©-requisitos

VocÃª precisa ter o Python 3.9 (ou superior) e o pip instalados. Recomenda-se o uso de um ambiente virtual.

```sh
# Crie um ambiente virtual
python -m venv venv

# Ative o ambiente virtual
# Windows
.\venv\Scripts\activate
# Linux/macOS
source venv/bin/activate
```

### InstalaÃ§Ã£o

1.  Clone o repositÃ³rio:
    ```sh
    git clone https://github.com/Joao-Victor-Leite/libras_CNN-RNN.git
    cd libras_CNN-RNN
    ```

2.  Instale as dependÃªncias a partir do arquivo `requirements.txt` (crie um se nÃ£o houver):
    ```sh
    pip install opencv-python mediapipe tensorflow numpy scikit-learn matplotlib seaborn jupyter
    ```

---

## ğŸƒ Uso

O projeto Ã© dividido em trÃªs etapas principais:

### 1. Coleta de Dados

Para treinar um novo modelo, vocÃª precisa primeiro coletar os dados. Execute o script `video_collector.py`. Ele irÃ¡ guiar vocÃª na gravaÃ§Ã£o de vÃ­deos curtos para cada letra do alfabeto, salvando os pontos-chave extraÃ­dos na estrutura de pastas correta dentro de `dataset/lstm/`.

```sh
python hand_capture/video_collector.py
```

### 2. Treinamento do Modelo

Com os dados coletados, vocÃª pode treinar o modelo LSTM. Abra e execute o notebook `train_lstm.ipynb` usando o Jupyter ou Google Colab, por utilizar o poder de processamento dos servidores do Google.

```sh
jupyter notebook main/train_lstm.ipynb
```

O notebook irÃ¡ processar os dados, construir a arquitetura do modelo, treinÃ¡-lo e salvar o arquivo `.h5` final no diretÃ³rio `models/`.

### 3. InferÃªncia em Tempo Real

ApÃ³s treinar e salvar um modelo, execute o script `lstm_inference.py` para iniciar o reconhecimento em tempo real. O script listarÃ¡ os modelos disponÃ­veis na pasta `models/` e pedirÃ¡ que vocÃª escolha qual carregar.

```sh
python main/lstm_inference.py
```

---

## ğŸ“ LicenÃ§a

DistribuÃ­do sob a LicenÃ§a MIT. Veja `LICENSE` para mais informaÃ§Ãµes.

---
